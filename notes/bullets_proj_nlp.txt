- - - - - - - ― - - -
milestone 
general
1. extract features for both the train and dev sets. (simple_fe.py)
2. train a model on the training set. (crfsuite learn -m mymodel train.feats)
3. make predictions on the devset, using the model we just trained. (crfsuite tag -m mymodel dev.feats > predtags)
4. evaluate the pretags against the gold standard tags of the devset. (python tageval.py dev.txt predtags)

use what software skill/programming language/tool/theory
1. part-of-speech tags
2. IOB2 notation
3. crfsuite

specific module (outputs data)
1. f-score
1’. training set (train.txt )
2’. development set (dev.txt )
3’. a simple feature extractor designed to be used with crfsuite (simple_fe.py )
4’. the evaluation script (tageval.py)

problems met?
1. F = 0.0362, prec = 0.75 (12/16) ([pr01's0101(05)n]), rec = 0.0185 (12/647)
there are 647 gold spans, we only found 12 of them, which is the main reason why we have such small F-score.
2. we get good predictions on the words or phrase which appeared in train.txt. This is reasonable considering that our codes right now only take the words around one certain word to be its attributes, and we assign them weights regarding the distance. But by doing this way it is hard to predict for unseen word in a new file. So we need to add new attributes to analyze structure of the sentences.
3. we only take word and relative position into consideration and this makes prediction bad.
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
final
general
1. data and evaluation (shared task or bakeoff competition)
1) initial training and development set provided
which contains a series of words, and chunk labels separated by space characters.

2) an evaluation script can test my tagger on the development set, on my own machine.
3) also can submit my development set predictions to the Kaggle, which can evaluate them for me.
4) to simulate a real situation, the final evaluation was done with a test set I did not have access to until the day of. I care about how my system does on new data it sees in the future.

use what software skill/programming language/tool/theory
1. part-of-speech tags
2. chunk labels: IOB2 notation
3. crfsuite, a software package that does first-order CRF sequence tagging.
4. python

specific module (output data)
background: in current research, system typically get F-scores in the 0.6 range.
milestone: created a very bare-bones tagger for the NER task.
outputs: 
1. system building
1) core system with lexical, character affix, shape features, positional offset versions
2) extensions of new features
2. analysis and exploration.

report:
the approaches I used features in NLP systems, and I found them some helpful and other not.
1. lexical/wordform 
1) Good for you. You are good. all become lower case. 
2) very helpful.
2. character affixes 
1) first 1, 2, 3 characters and the last 1, 2, 3 characters
2) helpful
3. shape features 
1) ”Twitter" to "Aa" and "Twitter3" to "AaD".
2) helpful4. positional offset versions of above. 
1) whether it holds for the word before or after
2) helpful

5. Compared to results from a CRFSuite-based system.
1) use a different ML algorithm or tagging model software. 
2) use external lexical resources for additional features.
3) use the unlabeled data.
4) use separate NLP tools to produce features or outputs.
- - - - - - - - - - - - - - - - - - - - - - -
+ Constructed an NER(Name Entity Recognition) tagger for Twitter to recognize spans of text that correspond to a name in tokenized tweets.

1. project general description

+ Implemented a feature extractor in Python to extract the characteristics of words, including lexical, character affix, shape features, and positional offset versions.

1. feature generation is the most important process, greatly affects the labeling accuracy.

+ Utilized the CRFsuite software package and the IOB notation to train a model on the training corpus and make predictions on the development corpus.

1. we used the IOB notation to represent a chunk (a span of tokens) with labels.
B represents a begin of a chunk, I represents an inside of a chunk.
e.g. Empire State Building = ESB.
tokens: ‘Empire’, ‘State’, ‘Building’, ‘=‘, ‘ESB’, ‘.’.
labels: B, I, I, O, B, O.
spans: (B, I, I), (O), (B), (O)
2. crfsuite is a software package that does first-order CRF sequence tagging. 
we need to implement the conversion from training/testing data to crfsuite data.

+ Designed a Python script to evaluate the predicted tags against the gold standard tags of the development corpus based on F-score.

1. our evalutation works at the span level.
2. span format follows (label, straitened, end index)
3. f-score is the average of precision and recall.

+ Optimized F-score from 0.036 to 0.475 for the previous development corpus, 0.362 for the unlabeled new tweets on Kaggle.com.

story
It is a cool data science project that consists many NLP knowledges to learn and practice. In general, the core part of this project is to build a system which can correctly recognize the name entities in tokenized tweets. 
To achieve this goal, 
+ we used two txt files which contains a series of words, and chunk labels as our training set and development set, 

e.g. Empire State Building = ESB.
tokens: ‘Empire’, ‘State’, ‘Building’, ‘=‘, ‘ESB’, ‘.’. dot
labels: B, I, I, O, B, O.

+ we designed a Python script as our feature extractor to extract characteristics of words in corpus, 
such as:
1. whether there is a same word appears in many times with different forms, 
(with its first letter capital in sentence head and all lower cased characters in the body)
2. what are the first 1, 2, 3 characters and last 1, 2, 3 characters in this word, 
3. what is the relationship between uppercase and lowercase characters in this word,
4. what are other words surrounding this word.
These features are helpful. Of course, we also tried external lexical resources for additional features. However they did not help much. Some took lots of time to train a model and even can be days, and some did not make much change for the f-score.

+ we used crfsuite this software package as our modeling tool and used f-score as our evaluation standard. 
+ we also submitted our development set predictions to the Kaggle to test how our system does on new data. 

In fact, the f-score is only 0.0036 (zero point double zero three six) at start, which is  very low. And we went to figure it out. After reading the outputs of evaluation, we found our precision was twelve out of sixteenth, recall was twelve out of six hundred forty seventh, there are 647 gold spans, we only found 12 of them, which was the main reason why we have such small f-score. We also found we had a good prediction on words and phrases which appeared in training set, but hard to predict correctly for unseen word in a new corpus. That was because we only took the word and its relative position into consideration. So we paid most of our effort to enrich feature extractor. 